{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1849c5",
   "metadata": {},
   "source": [
    "# This Python script was used to generate the machine learning models and figures used in the paper in the ROBIN RNA binders vs BindingDB classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56d402",
   "metadata": {},
   "source": [
    "## Software License:\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 Yazdani et al. \n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8806e",
   "metadata": {},
   "source": [
    "## Jupyter notebook within the Anaconda platform was used in writing the following script. Python version 3.8.2 was used as the coding language. \n",
    "$\\;\\;\\;\\;\\;\\;$\n",
    "\n",
    "\n",
    "\n",
    "![Python logo](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/300px-Python-logo-notext.svg.png)\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7eaede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "from platform import python_version\n",
    "print(\"Python version:\")\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddef0d4",
   "metadata": {},
   "source": [
    "## macOS Catalina version 10.15.7 was used when running this code on the Anaconda platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215fdef",
   "metadata": {},
   "source": [
    "## This code was written with the following package versions. Please install the following packages with the mentioned versions for consistency of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb34efe",
   "metadata": {},
   "source": [
    "### pandas --> version 1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb15e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pandas version\n",
    "import pandas as pd\n",
    "print(\"pandas version:\")\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86946629",
   "metadata": {},
   "source": [
    "### numpy --> version 1.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numpy version\n",
    "import numpy as np\n",
    "print(\"numpy version:\")\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32681cf1",
   "metadata": {},
   "source": [
    "### matplotlib --> version 3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b815a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check matplotlib version\n",
    "import matplotlib as plt\n",
    "print(\"matplotlib version:\")\n",
    "print(plt.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd5725",
   "metadata": {},
   "source": [
    "### seaborn --> version 0.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dddf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check seaborn version\n",
    "import seaborn as sns\n",
    "print(\"seaborn version:\")\n",
    "print(sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee570b",
   "metadata": {},
   "source": [
    "### sklearn --> version 1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67498cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sklearn version\n",
    "import sklearn\n",
    "print(\"sklearn version:\")\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d2b15",
   "metadata": {},
   "source": [
    "### tensorflow --> version 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c436571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tensorflow version\n",
    "import tensorflow as tf\n",
    "print(\"tensorflow version:\")\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4ce59",
   "metadata": {},
   "source": [
    "### keras --> version 2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check keras version\n",
    "print(\"keras version:\")\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c2efd",
   "metadata": {},
   "source": [
    "### shap --> version 0.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shap version\n",
    "import shap\n",
    "print(\"shap version:\")\n",
    "print(shap.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c390838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove at the end\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on\n",
    "# %pycodestyle_off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb813d",
   "metadata": {},
   "source": [
    "# Classification of ROBIN RNA binders and BindingDB protein binders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3531743",
   "metadata": {},
   "source": [
    "## a) Classification of ROBIN RNA binders and BindingDB protein binders (No Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6159590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages for data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Gets current directory of the jupyter notebook to use for directories of different files\n",
    "folder_dir = os.getcwd()[:-10]\n",
    "\n",
    "# Read in ROBIN RNA binders and BindingDB protein binders\n",
    "df_ROBIN_RNA = pd.read_csv(folder_dir + \"/data/Mordred_files/Mordred_ROBIN_RNA_Binder_3D.csv\")\n",
    "df_BindingDB = pd.read_csv(folder_dir + \"/data/Mordred_files/Mordred_BindingDB_3D.csv\")\n",
    "\n",
    "# Remove the name column\n",
    "df_ROBIN_RNA = df_ROBIN_RNA.drop('name', axis=1)\n",
    "df_BindingDB = df_BindingDB.drop('name', axis=1)\n",
    "\n",
    "# Delete duplicate rows that might still remain in the feature sets\n",
    "df_ROBIN_RNA.drop_duplicates(keep=\"first\", inplace=True)\n",
    "df_BindingDB.drop_duplicates(keep=\"first\", inplace=True)\n",
    "\n",
    "# ________________________________________________\n",
    "\n",
    "# Report the number of compounds in each library\n",
    "print(\"Number of compounds in the BindingDB protein binding library:\")\n",
    "print(df_BindingDB.shape[0])\n",
    "print()\n",
    "\n",
    "print(\"Number of compounds in the ROBIN RNA Binding library:\")\n",
    "print(df_ROBIN_RNA.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4369475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes and conduct further processing \n",
    "\n",
    "\"\"\"\n",
    "This cell might take a long time to run. This code does not have to be rerun if \n",
    "ROBIN_RNA_vs_BindingDB.csv is already saved in the ML_CSVs folder. \n",
    "\"\"\"\n",
    "\n",
    "# Add libary names to the dataframes\n",
    "df_ROBIN_RNA[\"Library\"] = \"RNA Binding\"\n",
    "df_BindingDB[\"Library\"] = \"Protein Binding\" \n",
    "\n",
    "# Concat the dataframes and convert feature columns to numeric\n",
    "df_all_2 = pd.concat([df_ROBIN_RNA, df_BindingDB])\n",
    "df_all_2_columns = df_all_2.columns.tolist()[:-1]\n",
    "df_all_2[df_all_2_columns] = df_all_2[df_all_2_columns].apply(pd.to_numeric,errors='coerce')\n",
    "\n",
    "print(\"Done 1\")\n",
    "\n",
    "# Delete columns that have feature values missing in more than 40% of rows\n",
    "df_all_2 = df_all_2.dropna(thresh=df_all_2.shape[0]*0.6, how='all',axis=1)\n",
    "\n",
    "print(\"Done 2\")\n",
    "\n",
    "# Replace missing and infinity values by median of each column\n",
    "df_all_2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_all_2 = df_all_2.fillna(df_all_2.median(numeric_only=True))\n",
    "\n",
    "# Reset the index\n",
    "df_all_2 = df_all_2.reset_index(drop=True)\n",
    "\n",
    "df_all_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3359861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file to the ML_CSVs folder so the preprocessing code does not have to be rerun multiple times\n",
    "\"\"\"\n",
    "Skip this cell if ROBIN_RNA_vs_BindingDB.csv is already saved in the ML_CSVs folder. \n",
    "\"\"\"\n",
    "df_all_2.to_csv(folder_dir + \"/ML_CSVs/ROBIN_RNA_vs_BindingDB.csv\", \n",
    "                index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95342fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If ROBIN_RNA_vs_BindingDB.csv is already saved in the ML_CSVs folder, skip the previous two cells\n",
    "and read the file ROBIN_RNA_vs_BindingDB.csv here. \n",
    "\"\"\"\n",
    "df_all_2 = pd.read_csv(folder_dir + \"/ML_CSVs/ROBIN_RNA_vs_BindingDB.csv\")\n",
    "df_all_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18924cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for ML by conversion to float numpy arrays\n",
    "\n",
    "# Prepare the features\n",
    "X_all_2 = df_all_2.values[:, :-1]\n",
    "X_all_2 = X_all_2.astype('float64')\n",
    "print(\"Shape of the X_all_2 matrix:\")\n",
    "print(X_all_2.shape)\n",
    "print()\n",
    "\n",
    "# Prepare the labels\n",
    "y_all_2 = df_all_2.values[:, -1]\n",
    "y_all_2 = y_all_2.tolist()\n",
    "y_all_2 = [1 if x == \"RNA Binding\" else x for x in y_all_2]\n",
    "y_all_2 = [0 if x == \"Protein Binding\" else x for x in y_all_2]\n",
    "y_all_2 = np.array(y_all_2)\n",
    "y_all_2 = y_all_2.astype(float)\n",
    "print(\"Shape of the y_all_2 labels array:\")\n",
    "print(y_all_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 90% of the data to cross validation (stratified) and save the remaining 10% for a final holdout(test) set.\n",
    "\n",
    "# Import splitting package\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all_2_CV, X_all_2_holdout, y_all_2_CV, y_all_2_holdout = train_test_split(X_all_2, y_all_2,\n",
    "                                                                            shuffle=True, test_size=0.1, \n",
    "                                                                            random_state=10, stratify=y_all_2)\n",
    "\n",
    "print(\"Shape of X_all_2 matrix:\")\n",
    "print(X_all_2.shape)\n",
    "print()\n",
    "\n",
    "print(\"Shape of the y_all_2 labels array:\")\n",
    "print(y_all_2.shape)\n",
    "print()\n",
    "\n",
    "print(\"Shape of X_all_2_CV matrix:\")\n",
    "print(X_all_2_CV.shape)\n",
    "print()\n",
    "\n",
    "print(\"Shape of the y_all_2_CV labels array:\")\n",
    "print(y_all_2_CV.shape)\n",
    "print()\n",
    "\n",
    "print(\"Shape of X_all_2_holdout matrix:\")\n",
    "print(X_all_2_holdout.shape)\n",
    "print()\n",
    "\n",
    "print(\"Shape of the y_all_2_holdout labels array:\")\n",
    "print(y_all_2_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6aee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data standardization before the LASSO logistic regression model\n",
    "# Data used for cross-validation will be standardized together for consistency of the data between cross-validation folds\n",
    "# The holdout(test) set is standardized using the standardscalar() fit to the X_all_2_CV array.\n",
    "# The holdout set was not standardized together with the X_all_2_CV array to prevent any data leakage.\n",
    "# After model selection and optimization using 10-fold cross-validation, the selected model is trained on the X_all_2_CV and tested on the holdout set. \n",
    "\n",
    "# Import standardization package\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the cross validation features\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_all_2_CV)\n",
    "X_all_2_CV = sc.transform(X_all_2_CV)\n",
    "print(\"Shape of the X_all_2_CV features array:\")\n",
    "print(X_all_2_CV.shape)\n",
    "print()\n",
    "\n",
    "# Standardize the holdout set features\n",
    "X_all_2_holdout = sc.transform(X_all_2_holdout)\n",
    "print(\"Shape of the X_all_2_holdout features array:\")\n",
    "print(X_all_2_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf72acb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell draws a PR curve with cross validation on the ROBIN RNA binders vs BindingDB classification with LASSO\n",
    "\n",
    "# Import ML packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Import and reset matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "\n",
    "# Change font to Helvetica\n",
    "plt.rc('font', family='Helvetica')\n",
    "\n",
    "# Import required packages for the PR curve\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "# Set seed value\n",
    "RANDOM_SEED = 5\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Define the logistic regression model\n",
    "clf = LogisticRegression(C=0.01, penalty='l1', solver='liblinear',\n",
    "                         max_iter=100, random_state=RANDOM_SEED, class_weight='balanced')\n",
    "\n",
    "# Split into 10 stratified folds for cross validation\n",
    "cv = StratifiedKFold(10, random_state=5, shuffle=True)\n",
    "\n",
    "# Define the colors to be used for the 10 folds as they are drawn on the PR curve\n",
    "colors = [\"lightblue\", \"r\", \"c\", \"m\", \"y\", \"lime\", \"mediumpurple\", \"olive\", \"aqua\", \"magenta\",\n",
    "          \"skyblue\", \"yellowgreen\", \"gold\", \"tan\", \"silver\"]\n",
    "\n",
    "# Cross validation for loop\n",
    "LR_y_real = []\n",
    "LR_y_proba = []\n",
    "num = 0\n",
    "for train, test in cv.split(X_all_2_CV, y_all_2_CV):\n",
    "\n",
    "    # Fit the model on the training set and predict probabilities for the train and test sets\n",
    "    LR_fit = clf.fit(X_all_2_CV[train], y_all_2_CV[train])\n",
    "    LR_train_probas = clf.predict_proba(X_all_2_CV[train])\n",
    "    LR_test_probas = clf.predict_proba(X_all_2_CV[test])\n",
    "\n",
    "    # Compute PR curve\n",
    "    precision, recall, _ = precision_recall_curve(y_all_2_CV[test], LR_test_probas[:, 1])\n",
    "\n",
    "    # Plotting each individual PR Curve and append them to LR_y_real and LR_y_proba lists\n",
    "    plt.plot(recall, precision, color=colors[num], lw=1, alpha=0.2)\n",
    "    LR_y_real.append(y_all_2_CV[test])\n",
    "    LR_y_proba.append(LR_test_probas[:, 1])\n",
    "\n",
    "    # Update num to keep track of fold number\n",
    "    num += 1\n",
    "\n",
    "    # Print Shapes and average precision score for each training and test round\n",
    "    print(\"Fold \" + str(num) + \":\")\n",
    "    print(\"Train and Test sets shapes:\")\n",
    "    print(X_all_2_CV[train].shape)\n",
    "    print(y_all_2_CV[train].shape)\n",
    "    print(X_all_2_CV[test].shape)\n",
    "    print(y_all_2_CV[test].shape)\n",
    "    print(\"Training set AUPRC:\")\n",
    "    print(round(average_precision_score(y_all_2_CV[train], LR_train_probas[:, 1]), 2))\n",
    "    print(\"Test set AUPRC:\")\n",
    "    print(round(average_precision_score(y_all_2_CV[test], LR_test_probas[:, 1]), 2))\n",
    "    print()\n",
    "\n",
    "    # Get the LR coefficients and sort them\n",
    "    LR_coeffs = clf.coef_[0].tolist()\n",
    "\n",
    "    # Calculate the number of positive and negative logistic regression coefficients\n",
    "    Num_nonzero_LR_coeffs = 0\n",
    "    for item in LR_coeffs:\n",
    "        if item != 0:\n",
    "            Num_nonzero_LR_coeffs += 1\n",
    "    print(\"Num_nonzero_LR_coeffs:\")\n",
    "    print(Num_nonzero_LR_coeffs)\n",
    "    \n",
    "    print()\n",
    "    print(\"*******************\")\n",
    "    print()\n",
    "\n",
    "# Plot the no skill line on the bottom of the PR curve\n",
    "no_skill = len(y_all_2_CV[y_all_2_CV == 1]) / len(y_all_2_CV)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill', color='red')\n",
    "\n",
    "# Concatenate all LR_y_real and LR_y_proba arrays for drawing of the mean curve\n",
    "LR_y_real = np.concatenate(LR_y_real)\n",
    "LR_y_proba = np.concatenate(LR_y_proba)\n",
    "\n",
    "# Calculate mean precision and recall for drawing the PR curve in the next step\n",
    "LR_precision, LR_recall, _ = precision_recall_curve(LR_y_real, LR_y_proba)\n",
    "\n",
    "# Plot the mean PR curve\n",
    "plt.plot(LR_recall, LR_precision, color='b', \n",
    "         label=r'LR: Mean AUPRC (K=10) = %0.2f' % (average_precision_score(LR_y_real, LR_y_proba)),\n",
    "         lw=2, alpha=1)\n",
    "\n",
    "# Set aspect ratio\n",
    "axes = plt.gca()\n",
    "axes.set_aspect(0.77)\n",
    "\n",
    "# Remove the gridlines\n",
    "plt.grid(visible=False)\n",
    "\n",
    "# Set x-axis and y-axis ticks\n",
    "tick_list = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "plt.xticks(tick_list, tick_list, fontsize=15)\n",
    "plt.yticks(tick_list, tick_list, fontsize=15)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.legend(loc=(0.05, 0.1), fontsize=13)\n",
    "\n",
    "# Save the PR curve\n",
    "plt.savefig(folder_dir + '/figures/PRC_ROBIN_RNA_vs_BindingDB.png', dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate highest and lowest coefficients of the LASSO logistic regression model\n",
    "\n",
    "# Train the logistic regression model on the entire dataset this time\n",
    "clf = LogisticRegression(C=0.01, penalty='l1', solver='liblinear', \n",
    "                         max_iter=100, random_state=RANDOM_SEED, class_weight='balanced').fit(X_all_2, y_all_2)\n",
    "\n",
    "# Get the LR coefficients and sort them\n",
    "LR_coeffs = clf.coef_[0].tolist()\n",
    "sorted_LR_coeffs = sorted(LR_coeffs, reverse=True)\n",
    "\n",
    "# Calculate the number of positive and negative logistic regression coefficients\n",
    "Num_positive_LR_coeffs = 0\n",
    "for item in LR_coeffs:\n",
    "    if item > 0:\n",
    "        Num_positive_LR_coeffs += 1\n",
    "Num_negative_LR_coeffs = 0\n",
    "for item in LR_coeffs:\n",
    "    if item < 0:\n",
    "        Num_negative_LR_coeffs += 1\n",
    "\n",
    "# Get the positive and negative coefficients as lists\n",
    "sorted_LR_coeffs_top = sorted_LR_coeffs[0:Num_positive_LR_coeffs]\n",
    "sorted_LR_coeffs_buttom = sorted_LR_coeffs[-1*Num_negative_LR_coeffs:][::-1]\n",
    "\n",
    "# Get features of the df_all_3 matrix\n",
    "features = df_all_2.columns.tolist()[1:-1]\n",
    "\n",
    "# Get the names of features with the highest positive coefficients in the LR model\n",
    "top_features = []\n",
    "for j in range(len(sorted_LR_coeffs_top)):\n",
    "    for i in range(len(LR_coeffs)):\n",
    "        if sorted_LR_coeffs_top[j] == LR_coeffs[i]:\n",
    "            top_features.append((features[i], round(LR_coeffs[i], 3)))\n",
    "print(\"Positive coefficients and their features:\")\n",
    "print(top_features)\n",
    "print()\n",
    "\n",
    "# Get the names of features with the lowest negative coefficients in the LR model\n",
    "buttom_features = []\n",
    "for j in range(len(sorted_LR_coeffs_buttom)):\n",
    "    for i in range(len(LR_coeffs)):\n",
    "        if sorted_LR_coeffs_buttom[j] == LR_coeffs[i]:\n",
    "            buttom_features.append((features[i], round(LR_coeffs[i], 3)))\n",
    "print(\"Negative coefficients and their features:\")\n",
    "print(buttom_features)\n",
    "\n",
    "\n",
    "# Report the number of non-zero features in the LR model\n",
    "count = 0\n",
    "for item in sorted_LR_coeffs:\n",
    "    if item > 0 or item < 0:\n",
    "        count += 1\n",
    "print()\n",
    "print(\"Number of nonzero coefficients:\")\n",
    "print(count)\n",
    "\n",
    "# Save the non-zero features and their coefficients in the supplemental folder\n",
    "df_LASSO_nonzero_features = pd.DataFrame(top_features+buttom_features, columns=['Feature', 'LR Coefficient'])\n",
    "df_LASSO_nonzero_features.to_csv(folder_dir +\n",
    "                                 '/LASSO_coefficients/LASSO_all_nonzero_features_ROBIN_RNA_vs_BindingDB.csv',\n",
    "                                 index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2789906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the entire CV set and test on the holdout set define earlier with relevant performance metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define the model\n",
    "clf = LogisticRegression(C=0.01, penalty='l1', solver='liblinear',\n",
    "                         max_iter=100, random_state=RANDOM_SEED, class_weight='balanced')\n",
    "\n",
    "# Fit the model on the training set and predict probabilities for the train and test sets\n",
    "LR_fit = clf.fit(X_all_2_CV, y_all_2_CV)\n",
    "prediction_probs = clf.predict_proba(X_all_2_holdout)[:, 1]\n",
    "prediction_classes = clf.predict(X_all_2_holdout)\n",
    "\n",
    "cm = confusion_matrix(y_all_2_holdout, prediction_classes)\n",
    "print(\"Confusion matrix on the holdout set:\")\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "AUPRC_score_holdout = average_precision_score(y_all_2_holdout, prediction_probs)\n",
    "print(\"AUPRC score on the holdout set:\")\n",
    "print(round(AUPRC_score_holdout, 2))\n",
    "print()\n",
    "\n",
    "f1_score_holdout = f1_score(y_all_2_holdout, prediction_classes)\n",
    "print(\"F1 score on the holdout set:\")\n",
    "print(round(f1_score_holdout, 2))\n",
    "print()\n",
    "\n",
    "AUROC_score_holdout = roc_auc_score(y_all_2_holdout, prediction_probs)\n",
    "print(\"AUROC score on the holdout set:\")\n",
    "print(round(AUROC_score_holdout, 2))\n",
    "print()\n",
    "\n",
    "cohen_kappa_score_holdout = cohen_kappa_score(y_all_2_holdout, prediction_classes)\n",
    "print(\"Cohenâ€™s kappa score on the holdout set:\")\n",
    "print(round(cohen_kappa_score_holdout, 2))\n",
    "print()\n",
    "\n",
    "matthews_corrcoef_holdout = matthews_corrcoef(y_all_2_holdout, prediction_classes)\n",
    "print(\"Matthews correlation coefficient on the holdout set:\")\n",
    "print(round(matthews_corrcoef_holdout, 2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a248c",
   "metadata": {},
   "source": [
    "## b) Classification of augmented ROBIN RNA binders and BindingDB protein binders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ca8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part of the code, we classify augmented ROBIN RNA and BindingDB protein binders.\n",
    "\n",
    "# Read in the ROBIN RNA binders, ROBIN augmented library, and BindingDB protein binders\n",
    "df_ROBIN_RNA = pd.read_csv(folder_dir + \"/data/Mordred_files/Mordred_ROBIN_RNA_Binder_3D.csv\")\n",
    "df_Augmented_ROBIN = pd.read_csv(folder_dir + \"/data/Mordred_files/Mordred_Augmented_ROBIN_RNA_3D.csv\")\n",
    "df_BindingDB = pd.read_csv(folder_dir + \"/data/Mordred_files/Mordred_BindingDB_3D.csv\")\n",
    "\n",
    "# ______________________________________________\n",
    "\n",
    "# Report the number of compounds in each library that will be used in machine learning\n",
    "print(\"Number of compounds in the BindingDB protein binding library:\")\n",
    "print(df_BindingDB.shape[0])\n",
    "print()\n",
    "\n",
    "print(\"Number of compounds in the Augmented_ROBIN library:\")\n",
    "print(df_Augmented_ROBIN.shape[0])\n",
    "print()\n",
    "\n",
    "print(\"Number of compounds in the ROBIN RNA Binding library:\")\n",
    "print(df_ROBIN_RNA.shape[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405d5ad",
   "metadata": {},
   "source": [
    "### Machine Learning Strategy:\n",
    "\n",
    "Two matrices wil be utilized to train the augmented ROBIN RNA binders vs BindingDB classifiers.\n",
    "\n",
    "Matrix 1 (df_all_3): This matrix is comprised of augmented ROBIN RNA binders and BindingDB protein binders. This is the matrix whose standardized chemical descriptor values will be used throughout for training and testing the dataset. \n",
    "\n",
    "Matrix 2 (df_all_4): This matrix only has experimentally-derived ROBIN RNA binders and BindingDB protein binders and their features. This matrix does not have any computational augmentation at any stage. This matrix is used for splitting the dataset and defining the train and test sets in each round of cross-validation. Only compound names of this matrix are used in the machine learning steps. \n",
    "\n",
    "### 10-fold Cross-Validation Strategy Step by Step:\n",
    "\n",
    "1. Matrix df_all_4 (matrix 2) is split in a stratified manner (same class balance as the original dataset) into 10 folds. Splitting of the matrix that is not augmented ensures only experimenal compounds end up in each test set. \n",
    "2. In each round of cross validation, one of the 10 folds is assigned as a test set.\n",
    "3. Compounds in each of the test sets are first extracted. Then, BindingDB compounds of the test set are taken out of df_all_3. For ROBIN RNA binders in each test set, the compounds themselves and all their analogues are taken out of df_all_3 to prevent any data leakage from train to test set.\n",
    "4. The new df_all_3 with the test set compounds and their analogues taken out is trained on with the machine learning models. Models are tested on the test set compounds extracted from the experimental df_all_4 matrix. All features are standardized initially in the df_all_3 matrix so rows of compound features are exclusively extracted from the df_all_3 matrix.\n",
    "\n",
    "** Note: As a result of this cross-validation strategy, the train and test sets have different class balances; therefore, higher performance of the model on the training set might not be indicative of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f75261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes and conduct further processing\n",
    "\n",
    "\"\"\"\n",
    "This cell might take a long time to run. This code does not have to be rerun if \n",
    "Augmented_ROBIN_RNA_vs_BindingDB.csv is already saved in the ML_CSVs folder. \n",
    "\"\"\"\n",
    "\n",
    "# Assigning a library name to the two dataframes\n",
    "df_BindingDB[\"library\"] = \"Protein Binding\"\n",
    "df_Augmented_ROBIN[\"library\"] = \"RNA Binding\"\n",
    "\n",
    "# Concat the two dataframes\n",
    "df_all_3 = pd.concat([df_BindingDB, df_Augmented_ROBIN])\n",
    "\n",
    "# Get the list of all feature columns in the dataset and coerce them to numeric\n",
    "df_all_3_columns = df_all_3.columns.tolist()[1:-1]\n",
    "df_all_3[df_all_3_columns] = df_all_3[df_all_3_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(\"Done1\")\n",
    "# Delete columns that have feature values missing in more than 60% of compounds\n",
    "df_all_3 = df_all_3.dropna(thresh=df_all_3.shape[0]*0.6, how='all', axis=1)\n",
    "\n",
    "print(\"Done2\")\n",
    "# Replace missing and infinity values by median of each column\n",
    "df_all_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_all_3 = df_all_3.fillna(df_all_3.median(numeric_only=True))\n",
    "\n",
    "# Reset the index\n",
    "df_all_3 = df_all_3.reset_index(drop=True)\n",
    "df_all_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed dataframe so the data does not have to reprocessed with every notebook\n",
    "\"\"\"\n",
    "Skip this cell if Augmented_ROBIN_RNA_vs_BindingDB.csv is already saved in the ML_CSVs folder. \n",
    "\"\"\"\n",
    "df_all_3.to_csv(folder_dir + \"/ML_CSVs/Augmented_ROBIN_RNA_vs_BindingDB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975cff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load back the preprocessed dataframe\n",
    "\"\"\"\n",
    "If Augmented_ROBIN_RNA_vs_BindingDB.csv is already saved in the ML_CSVs folder, skip the previous two cells\n",
    "and read the file Augmented_ROBIN_RNA_vs_BindingDB.csv here.\n",
    "\"\"\"\n",
    "df_all_3 = pd.read_csv(folder_dir + \"/ML_CSVs/Augmented_ROBIN_RNA_vs_BindingDB.csv\")\n",
    "df_all_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and standardization before the LASSO logistic regression and neural network models\n",
    "# df_all_3 is the augmented dataset combined\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare the features\n",
    "X_all_3 = df_all_3.values[:, 1:-1]\n",
    "X_all_3 = X_all_3.astype('float64')\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_all_3)\n",
    "X_all_3 = sc.transform(X_all_3)\n",
    "print(\"Shape of X_all_3 matrix:\")\n",
    "print(X_all_3.shape)\n",
    "print()\n",
    "\n",
    "# Prepare the labels\n",
    "y_all_3 = df_all_3.values[:, -1]\n",
    "y_all_3 = y_all_3.tolist()\n",
    "y_all_3 = [1 if x == \"RNA Binding\" else x for x in y_all_3]\n",
    "y_all_3 = [0 if x == \"Protein Binding\" else x for x in y_all_3]\n",
    "y_all_3 = np.array(y_all_3)\n",
    "y_all_3 = y_all_3.astype(float)\n",
    "print(\"Shape of the y_all_3 labels array:\")\n",
    "print(y_all_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0811a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert standardized df_all_3 back to array since Pandas operations will be used on this dataframe later\n",
    "# Also, add the name and label columns back in as they will be used later\n",
    "df_all_3_feature_columns = df_all_3.columns.tolist()[1:-1]\n",
    "df_all_3_name_label_standardized = pd.DataFrame(X_all_3, columns=df_all_3_feature_columns)\n",
    "df_all_3_compound_names = pd.Series(df_all_3[\"name\"].tolist(), name=\"name\")\n",
    "df_all_3_name_label_standardized = pd.concat([df_all_3_compound_names, df_all_3_name_label_standardized], axis=1)\n",
    "df_all_3_labels = pd.Series(y_all_3.tolist(), name=\"label\")\n",
    "df_all_3_name_label_standardized = pd.concat([df_all_3_name_label_standardized, df_all_3_labels], axis=1)\n",
    "df_all_3_name_label_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0790bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproccess df_all_4 and save so the preprocessing does not have to be rerun\n",
    "# df_all_4 is the non-augmented dataset which will be used for testing the performance of the augmented set in each fold of cross validation\n",
    "\n",
    "\"\"\"\n",
    "This cell might take a long time to run. This code does not have to be rerun if \n",
    "ROBIN_RNA_vs_BindingDB_used_in_Augmentation.csv is already saved in the ML_CSVs folder. \n",
    "\"\"\"\n",
    "\n",
    "# Add libary names to the dataframes\n",
    "df_ROBIN_RNA[\"library\"] = \"RNA Binding\"\n",
    "df_BindingDB[\"library\"] = \"Protein Binding\" \n",
    "\n",
    "\n",
    "df_all_4 = pd.concat([df_ROBIN_RNA, df_BindingDB])\n",
    "\n",
    "df_all_3_features = df_all_3.columns.tolist()\n",
    "df_all_4 = df_all_4[df_all_3_features]\n",
    "\n",
    "df_all_4_features = df_all_4.columns.tolist()[1:-1]\n",
    "df_all_4[df_all_4_features] = df_all_4[df_all_4_features].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(\"Done1\")\n",
    "\n",
    "\n",
    "# Replace missing and infinity values by median of each column\n",
    "df_all_4.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_all_4 = df_all_4.fillna(df_all_4.median(numeric_only=True))\n",
    "\n",
    "\n",
    "print(\"Done2\")\n",
    "\n",
    "# Reset the index\n",
    "df_all_4 = df_all_4.reset_index(drop=True)\n",
    "\n",
    "df_all_4.to_csv(folder_dir + \"/ML_CSVs/ROBIN_RNA_vs_BindingDB_used_in_Augmentation.csv\", index=False)\n",
    "\n",
    "df_all_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If ROBIN_RNA_vs_BindingDB_used_in_Augmentation.csv is already saved in the ML_CSVs folder, skip the previous cell\n",
    "and read the file Augmented_ROBIN_RNA_vs_BindingDB.csv here. \n",
    "\"\"\"\n",
    "\n",
    "# Load back df_all_4 and standardize\n",
    "df_all_4 = pd.read_csv(folder_dir + \"/ML_CSVs/ROBIN_RNA_vs_BindingDB_used_in_Augmentation.csv\")\n",
    "#prepare the features\n",
    "X_all_4 = df_all_4.values[:, :-1]\n",
    "print(\"Shape of X_all_4 matrix:\")\n",
    "print(X_all_4.shape)\n",
    "print()\n",
    "\n",
    "#prepare the labels\n",
    "y_all_4 = df_all_4.values[:,-1]\n",
    "y_all_4 = y_all_4.tolist()\n",
    "y_all_4 = [1 if x==\"RNA Binding\" else x for x in y_all_4]\n",
    "y_all_4 = [0 if x==\"Protein Binding\" else x for x in y_all_4]\n",
    "y_all_4 = np.array(y_all_4)\n",
    "y_all_4 = y_all_4.astype(float)\n",
    "print(\"Shape of the y_all_4 labels array:\")\n",
    "print(y_all_4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f62380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 90% of the data to cross validation (stratified) and save the remaining 10% for a final holdout(test) set. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all_4_CV, X_all_4_holdout, y_all_4_CV, y_all_4_holdout = train_test_split(X_all_4, y_all_4,shuffle = True, test_size=0.1, random_state=10, stratify = y_all_4)\n",
    "\n",
    "print (\"Shape of X_all_4 matrix:\")\n",
    "print (X_all_4.shape)\n",
    "print ()\n",
    "\n",
    "print (\"Shape of the y_all_4 labels array:\")\n",
    "print (y_all_4.shape)\n",
    "print ()\n",
    "\n",
    "print (\"Shape of X_all_4_CV matrix:\")\n",
    "print (X_all_4_CV.shape)\n",
    "print ()\n",
    "\n",
    "print (\"Shape of the y_all_4_CV labels array:\")\n",
    "print (y_all_4_CV.shape)\n",
    "print ()\n",
    "\n",
    "print (\"Shape of X_all_4_holdout matrix:\")\n",
    "print (X_all_4_holdout.shape)\n",
    "print ()\n",
    "\n",
    "print (\"Shape of the y_all_4_holdout labels array:\")\n",
    "print (y_all_4_holdout.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38519cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dictionary that maps each ROBIN RNA binder to 30 analogues for augmentation \n",
    "# This dictionary will be used to prevent data leakage during cross-validation to exclude analogues of test cases in the train set\n",
    "df_ROBIN_Augment_map = pd.read_csv(folder_dir + \"/ML_CSVs/ROBIN_Plus_Dictionary.csv\")\n",
    "df_ROBIN_Augment_map_names_og = df_ROBIN_Augment_map[\"Name\"].tolist()\n",
    "df_ROBIN_Augment_map_names = df_ROBIN_Augment_map[\"ROBIN_Plus_Name\"].tolist()\n",
    "df_ROBIN_Augment_map_names_augmented = []\n",
    "for item in df_ROBIN_Augment_map_names:\n",
    "    df_ROBIN_Augment_map_names_augmented.append(item.split(\";\"))\n",
    "ROBIN_Augment_dict = {}\n",
    "for i in range(len(df_ROBIN_Augment_map_names_og)):\n",
    "    ROBIN_Augment_dict[df_ROBIN_Augment_map_names_og[i]] =  df_ROBIN_Augment_map_names_augmented[i]\n",
    "ROBIN_Augment_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1bbb30",
   "metadata": {},
   "source": [
    "## Next step removes the holdout set compounds from the set used for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea2091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude holdout(test) set compounds and their analogues from the set of compounds used for cross-validation\n",
    "# This operation will be performed on the df_all_3_name_label_standardized dataframe.\n",
    "\n",
    "# RNA compounds to exclude from the CV set\n",
    "test_set_compounds = X_all_4_holdout[:, 0].tolist()\n",
    "test_set_RNA_compounds = [item for item in test_set_compounds if item[0:9] != \"BindingDB\"]\n",
    "test_set_RNA_compounds_expanded = []\n",
    "for item in test_set_RNA_compounds:\n",
    "    test_set_RNA_compounds_expanded.append(item)\n",
    "    item_expanded = ROBIN_Augment_dict[item]\n",
    "    for thing in item_expanded:\n",
    "        test_set_RNA_compounds_expanded.append(thing)\n",
    "test_set_RNA_compounds_expanded_set = list(set(test_set_RNA_compounds_expanded))\n",
    "\n",
    "# BindingDB compounds to exclude from the CV set\n",
    "test_set_BindingDB = [item for item in test_set_compounds if item[0:9] == \"BindingDB\"]\n",
    "\n",
    "# BindingDB and RNA binders to be excluded from the CV set combined into one set\n",
    "test_set_RNA_expanded_and_BindingDB = test_set_BindingDB + test_set_RNA_compounds_expanded_set\n",
    "\n",
    "# Remove from df_all_3_name_label_standardized BindingDB compounds from the test set\n",
    "# Also remove from df_all_3_name_label_standardized test set ROBIN RNA binders and their analogues\n",
    "df_all_3_name_label_standardized_CV = df_all_3_name_label_standardized.loc[~df_all_3_name_label_standardized['name'].isin(test_set_RNA_expanded_and_BindingDB)]\n",
    "# df_all_3_name_label_standardized_CV is the cross-validation set\n",
    "\n",
    "# Print shape of the CV set and the number of protein and RNA binders within it\n",
    "print(\"Shape of the augmented cross-validation set:\")\n",
    "print(df_all_3_name_label_standardized_CV.shape)\n",
    "y_all_3_CV_labels_list = df_all_3_name_label_standardized_CV.values[:, -1].tolist()\n",
    "y_all_3_CV_labels_list = [1 if x == \"RNA Binding\" else x for x in y_all_3_CV_labels_list]\n",
    "y_all_3_CV_labels_list = [0 if x == \"Protein Binding\" else x for x in y_all_3_CV_labels_list]\n",
    "print(\"Number of protein binders:\")\n",
    "print(int(len(y_all_3_CV_labels_list) - sum(y_all_3_CV_labels_list)))\n",
    "print(\"Number of RNA binders:\")\n",
    "print(int(sum(y_all_3_CV_labels_list)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc4e6e",
   "metadata": {},
   "source": [
    "## Next step does the splitting as described earlier and saves each train and test set for each fold in the ML_CSVs folder. This step will ensure the splitting does not have be to be redone after the initial run of the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the folds used for cross validation as CSVs for faster execution when code is run again\n",
    "\n",
    "\"\"\"\n",
    "This cell might take a long time to run. This code does not have to be rerun if \n",
    "CSVs of all folds are already saved in the Augmented_ROBIN_vs_BindingDB_Stratified_Folds folder within the \n",
    "ML_CSVs folder. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Split into 10 folds in a stratified manner \n",
    "cv = StratifiedKFold(10, random_state = 5, shuffle = True)\n",
    "\n",
    "num = 1\n",
    "for train, test in cv.split(X_all_4_CV, y_all_4_CV):\n",
    "    \n",
    "    # Print fold number \n",
    "    print (\"Fold \" + str(num) + \":\")\n",
    "\n",
    "    # RNA compounds to exclude from the training set\n",
    "    test_set_compounds = X_all_4_CV[test][:, 0].tolist()\n",
    "    test_set_RNA_compounds = [item for item in test_set_compounds if item[0:9] != \"BindingDB\"]\n",
    "    test_set_RNA_compounds_expanded = []\n",
    "    for item in test_set_RNA_compounds:\n",
    "        test_set_RNA_compounds_expanded.append(item)\n",
    "        item_expanded = ROBIN_Augment_dict[item]\n",
    "        for thing in item_expanded:\n",
    "            test_set_RNA_compounds_expanded.append(thing)\n",
    "    test_set_RNA_compounds_expanded_set = list(set(test_set_RNA_compounds_expanded))\n",
    "\n",
    "    # BindingDB compounds to exclude from the training set\n",
    "    test_set_BindingDB = [item for item in test_set_compounds if item[0:9] == \"BindingDB\"]\n",
    "\n",
    "    # BindingDB and RNA binders to be excluded from the train set combined into one set\n",
    "    test_set_RNA_expanded_and_BindingDB = test_set_BindingDB + test_set_RNA_compounds_expanded_set\n",
    "    \n",
    "    # Remove from df_all_3_name_label_standardized_CV BindingDB compounds from the test set\n",
    "    # Also remove from df_all_3_name_label_standardized_CV test set ROBIN RNA binders and their analogues\n",
    "    df_all_3_name_label_standardized_train = df_all_3_name_label_standardized_CV.loc[~df_all_3_name_label_standardized_CV['name'].isin(test_set_RNA_expanded_and_BindingDB)]\n",
    "    # df_all_3_name_label_standardized_train is the train set\n",
    "    \n",
    "    # Print shape of the train set and the number of protein and RNA binders within it\n",
    "    print(\"Shape of Augmented Train set:\")\n",
    "    print (df_all_3_name_label_standardized_train.shape)\n",
    "    df_all_3_name_label_standardized_train_labels = df_all_3_name_label_standardized_train[\"label\"].tolist()\n",
    "    print(\"Number of protein binders:\")\n",
    "    print(int(len(df_all_3_name_label_standardized_train_labels) - sum(df_all_3_name_label_standardized_train_labels)))\n",
    "    print(\"Number of RNA binders:\")\n",
    "    print(int(sum(df_all_3_name_label_standardized_train_labels)))\n",
    "    print()\n",
    "\n",
    "    # Extract from df_all_3_name_label_standardized_CV the test set compounds belonging to BindingDB and ROBIN RNA binders\n",
    "    test_set_RNA_and_BindingDB = test_set_RNA_compounds + test_set_BindingDB\n",
    "    df_all_3_name_label_standardized_test = df_all_3_name_label_standardized_CV.loc[df_all_3_name_label_standardized_CV['name'].isin(test_set_RNA_and_BindingDB)]\n",
    "    df_all_3_name_label_standardized_test.drop_duplicates(subset=['name'], keep=\"first\", inplace=True)\n",
    "    # df_all_3_name_label_standardized_test is the test set\n",
    "    # Test set compounds are extracted again from df_all_3_name_label_standardized_CV to ensure standardization of train and test sets are the same\n",
    "    \n",
    "    # Print shape of the test set and the number of protein and RNA binders within it\n",
    "    print(\"Shape of test set:\")\n",
    "    print(df_all_3_name_label_standardized_test.shape)\n",
    "    df_all_3_name_label_standardized_test_labels = df_all_3_name_label_standardized_test[\"label\"].tolist()\n",
    "    print(\"Number of protein binders:\")\n",
    "    print(int(len(df_all_3_name_label_standardized_test_labels) - sum(df_all_3_name_label_standardized_test_labels)))\n",
    "    print(\"Number of RNA binders:\")\n",
    "    print(int(sum(df_all_3_name_label_standardized_test_labels)))\n",
    "\n",
    "    # Get the feature values and labels from the pandas dataframes\n",
    "    X_3_all_Augmented_train = df_all_3_name_label_standardized_train.iloc[:, 1:-1]\n",
    "    y_3_all_Augmented_train = df_all_3_name_label_standardized_train.iloc[:, -1]\n",
    "\n",
    "    X_3_all_Augmented_test = df_all_3_name_label_standardized_test.iloc[:, 1:-1]\n",
    "    y_3_all_Augmented_test = df_all_3_name_label_standardized_test.iloc[:, -1]\n",
    "    \n",
    "    # Save in Augmented_ROBIN_vs_BindingDB_Stratified_Folds folder within the ML_CSVs folder\n",
    "    save_dir = folder_dir + \"/ML_CSVs/Augmented_ROBIN_vs_BindingDB_Stratified_Folds/\" + \"fold_\" + str(num) + \"/\"\n",
    "    \n",
    "    # Save the files in their corresponding CSVs\n",
    "    X_3_all_Augmented_train.to_csv(save_dir + \"X_train\" + \".csv\", index=False)\n",
    "    y_3_all_Augmented_train.to_csv(save_dir + \"y_train\" + \".csv\", index=False)\n",
    "    X_3_all_Augmented_test.to_csv(save_dir + \"X_test\" + \".csv\", index=False)\n",
    "    y_3_all_Augmented_test.to_csv(save_dir + \"y_test\" + \".csv\", index=False)\n",
    "    \n",
    "    #Update num\n",
    "    num += 1\n",
    "    \n",
    "    print ()\n",
    "    print(\"*****************\")\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ebf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If folds are already saved in the Augmented_ROBIN_vs_BindingDB_Stratified_Folds folder within the ML_CSVs folder, \n",
    "skip the previous cell and read each fold's train and test sets here. \n",
    "\"\"\"\n",
    "\n",
    "# Read the saved folds back in \n",
    "\n",
    "for k in range(1,11):\n",
    "    \n",
    "    print (k)\n",
    "    \n",
    "    exec(f'fold_{k}_X_train = pd.read_csv(folder_dir + \"/ML_CSVs/Augmented_ROBIN_vs_BindingDB_Stratified_Folds/\" + \"fold_\" + str(k) + \"/\" + \"X_train.csv\")')\n",
    "  \n",
    "    exec(f'fold_{k}_y_train = pd.read_csv(folder_dir + \"/ML_CSVs/Augmented_ROBIN_vs_BindingDB_Stratified_Folds/\" + \"fold_\" + str(k) + \"/\" + \"y_train.csv\")')\n",
    "\n",
    "    exec(f'fold_{k}_X_test = pd.read_csv(folder_dir + \"/ML_CSVs/Augmented_ROBIN_vs_BindingDB_Stratified_Folds/\" + \"fold_\" + str(k) + \"/\" + \"X_test.csv\")')\n",
    "\n",
    "    exec(f'fold_{k}_y_test = pd.read_csv(folder_dir + \"/ML_CSVs/Augmented_ROBIN_vs_BindingDB_Stratified_Folds/\" + \"fold_\" + str(k) + \"/\" + \"y_test.csv\")')\n",
    "\n",
    "# Example file read from fold_1_X_train\n",
    "fold_1_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c73b3e5",
   "metadata": {},
   "source": [
    "## LASSO Logistic Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on LASSO logistic regression model\n",
    "\n",
    "# Import required packages for the PR curve\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define the logistic regression model\n",
    "clf = LogisticRegression(C=0.005, penalty='l1', solver='liblinear',\n",
    "                         max_iter=100, random_state=RANDOM_SEED, class_weight='balanced')\n",
    "\n",
    "# Define the colors to be used for the 10 folds as they are drawn on the PR curve\n",
    "colors = [\"lightblue\", \"r\", \"c\", \"m\", \"y\", \"lime\", \"mediumpurple\", \"olive\", \"aqua\",\n",
    "          \"magenta\", \"skyblue\", \"yellowgreen\", \"gold\", \"tan\", \"silver\"]\n",
    "\n",
    "# Cross-validation for loop\n",
    "LR_y_real = []\n",
    "LR_y_proba = []\n",
    "num = 0\n",
    "for k in range(1, 11):\n",
    "\n",
    "    # Print fold number\n",
    "    print(\"Fold \" + str(num+1) + \":\")\n",
    "\n",
    "    # Read in the fetures and labels for the train and test sets read earlier\n",
    "    X_train = vars()['fold_' + str(k) + \"_X_train\"]\n",
    "    y_train = vars()['fold_' + str(k) + \"_y_train\"]\n",
    "    X_test = vars()['fold_' + str(k) + \"_X_test\"]\n",
    "    y_test = vars()['fold_' + str(k) + \"_y_test\"]\n",
    "\n",
    "    # Update num\n",
    "    num += 1\n",
    "\n",
    "    # Convert the dataframes to numpy arrays for machine learning\n",
    "    X_3_all_Augmented_train = X_train.values\n",
    "    y_3_all_Augmented_train = y_train.values\n",
    "    X_3_all_Augmented_test = X_test.values\n",
    "    y_3_all_Augmented_test = y_test.values\n",
    "\n",
    "    # Fit the model on the training set and predict probabilities for the train and test sets\n",
    "    LR_fit = clf.fit(X_3_all_Augmented_train, y_3_all_Augmented_train)\n",
    "    LR_test_probas = clf.predict_proba(X_3_all_Augmented_test)\n",
    "    LR_train_probas = clf.predict_proba(X_3_all_Augmented_train)\n",
    "\n",
    "\n",
    "    # Compute PR and area under the curve\n",
    "    precision, recall, _ = precision_recall_curve(y_3_all_Augmented_test, LR_test_probas[:, 1])\n",
    "\n",
    "    # Plotting each individual PR Curve and append them to LR_y_real and LR_y_proba lists\n",
    "    plt.plot(recall, precision, color=colors[num], lw=1, alpha=0.2)\n",
    "    LR_y_real.append(y_3_all_Augmented_test)\n",
    "    LR_y_proba.append(LR_test_probas[:, 1])\n",
    "\n",
    "    # Get model class predictions instead of probabilities to show the confusion matrices\n",
    "    prediction_classes_test = clf.predict(X_3_all_Augmented_test)\n",
    "    prediction_classes_train = clf.predict(X_3_all_Augmented_train)\n",
    "\n",
    "    # Draw confusion matrix of the model performance on the train set\n",
    "    cm = confusion_matrix(y_3_all_Augmented_train, prediction_classes_train)\n",
    "    print(\"Confusion matrix on the test set:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Draw confusion matrix of the model performance on the test set\n",
    "    cm = confusion_matrix(y_3_all_Augmented_test, prediction_classes_test)\n",
    "    print(\"Confusion matrix on the test set:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Print F1 score and average prevision score for train and test sets in each round\n",
    "    print(\"F1 score test:\")\n",
    "    print(round(f1_score(y_3_all_Augmented_test, prediction_classes_test), 2))\n",
    "    print(\"F1 score train:\")\n",
    "    print(round(f1_score(y_3_all_Augmented_train, prediction_classes_train), 2))\n",
    "    print()\n",
    "    print(\"Test AVG precision score:\")\n",
    "    print(round(average_precision_score(y_3_all_Augmented_test, LR_test_probas[:, 1]),2))\n",
    "    print(\"Train AVG precision score:\")\n",
    "    print(round(average_precision_score(y_3_all_Augmented_train, LR_train_probas[:, 1]),2))\n",
    "    print()\n",
    "\n",
    "    # Get the LR coefficients and sort them\n",
    "    LR_coeffs = clf.coef_[0].tolist()\n",
    "\n",
    "    # Calculate the number of positive and negative logistic regression coefficients\n",
    "    Num_nonzero_LR_coeffs = 0\n",
    "    for item in LR_coeffs:\n",
    "        if item != 0:\n",
    "            Num_nonzero_LR_coeffs += 1\n",
    "\n",
    "    # Report number of nonzero LR coefficients\n",
    "    print(\"Num_nonzero_LR_coeffs:\") \n",
    "    print(Num_nonzero_LR_coeffs)\n",
    "\n",
    "    print()\n",
    "    print(\"*******************\")\n",
    "    print()\n",
    "\n",
    "# Plot the no skill line on the bottom of the PR curve\n",
    "no_skill = len(y_all_2_CV[y_all_2_CV == 1]) / len(y_all_2_CV)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill', color='red')\n",
    "\n",
    "# Concatenate all LR_y_real and LR_y_proba arrays for drawing of the mean curve\n",
    "LR_y_real = np.concatenate(LR_y_real)\n",
    "LR_y_proba = np.concatenate(LR_y_proba)\n",
    "\n",
    "# Calculate mean precision and recall for drawing the PR curve in the next step\n",
    "LR_precision, LR_recall, thresholds = precision_recall_curve(LR_y_real, LR_y_proba)\n",
    "\n",
    "# Plot the mean PR curve\n",
    "plt.plot(LR_recall, LR_precision, color='b',\n",
    "             label=r'LR: Mean AUPRC (K=10) = %0.2f' % (average_precision_score(LR_y_real, LR_y_proba)),\n",
    "             lw=2, alpha=1)\n",
    "\n",
    "# Set aspect ratio\n",
    "axes = plt.gca()\n",
    "axes.set_aspect(0.75)\n",
    "\n",
    "# Remove the gridlines\n",
    "plt.grid(visible=False)\n",
    "\n",
    "\n",
    "# Set x-axis and y-axis ticks\n",
    "tick_list = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "plt.xticks(tick_list, tick_list, fontsize=15)\n",
    "plt.yticks(tick_list, tick_list, fontsize=15)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.legend(loc=\"lower left\", fontsize=13)\n",
    "\n",
    "\n",
    "# save the PR curve\n",
    "plt.savefig(folder_dir + '/figures/PRC_Augmented_ROBIN_RNA_vs_BindingDB_LR.png',\n",
    "            dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f012b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate highest and lowest coefficients of the LASSO logistic regression model\n",
    "\n",
    "# Train the logistic regression model on the entire dataset this time\n",
    "clf = LogisticRegression(C=0.005, penalty='l1', solver='liblinear', \n",
    "                         max_iter=100, random_state=RANDOM_SEED, class_weight='balanced').fit(X_all_3, y_all_3)\n",
    "\n",
    "# Get the LR coefficients and sort them\n",
    "LR_coeffs = clf.coef_[0].tolist()\n",
    "sorted_LR_coeffs = sorted(LR_coeffs, reverse=True)\n",
    "\n",
    "# Calculate the number of positive and negative logistic regression coefficients\n",
    "Num_positive_LR_coeffs = 0\n",
    "for item in LR_coeffs:\n",
    "    if item > 0:\n",
    "        Num_positive_LR_coeffs += 1\n",
    "Num_negative_LR_coeffs = 0\n",
    "for item in LR_coeffs:\n",
    "    if item < 0:\n",
    "        Num_negative_LR_coeffs += 1\n",
    "\n",
    "# Get the positive and negative coefficients as lists\n",
    "sorted_LR_coeffs_top = sorted_LR_coeffs[0:Num_positive_LR_coeffs]\n",
    "sorted_LR_coeffs_buttom = sorted_LR_coeffs[-1*Num_negative_LR_coeffs:][::-1]\n",
    "\n",
    "# Get features of the df_all_3 matrix\n",
    "features = df_all_3.columns.tolist()[1:-1]\n",
    "\n",
    "# Get the names of features with the highest positive coefficients in the LR model\n",
    "top_features = []\n",
    "for j in range(len(sorted_LR_coeffs_top)):\n",
    "    for i in range(len(LR_coeffs)):\n",
    "        if sorted_LR_coeffs_top[j] == LR_coeffs[i]:\n",
    "            top_features.append((features[i], round(LR_coeffs[i], 3)))\n",
    "print(\"Positive coefficients and their features:\")\n",
    "print(top_features)\n",
    "print()\n",
    "\n",
    "# Get the names of features with the lowest negative coefficients in the LR model\n",
    "buttom_features = []\n",
    "for j in range(len(sorted_LR_coeffs_buttom)):\n",
    "    for i in range(len(LR_coeffs)):\n",
    "        if sorted_LR_coeffs_buttom[j] == LR_coeffs[i]:\n",
    "            buttom_features.append((features[i], round(LR_coeffs[i], 3)))\n",
    "print(\"Negative coefficients and their features:\")\n",
    "print(buttom_features)\n",
    "\n",
    "\n",
    "# Report the number of non-zero features in the LR model\n",
    "count = 0\n",
    "for item in sorted_LR_coeffs:\n",
    "    if item > 0 or item < 0:\n",
    "        count += 1\n",
    "print()\n",
    "print(\"Number of nonzero coefficients:\")\n",
    "print(count)\n",
    "\n",
    "# Save the non-zero features and their coefficients in the supplemental folder\n",
    "df_LASSO_nonzero_features = pd.DataFrame(top_features+buttom_features, columns=['Feature', 'LR Coefficient'])\n",
    "df_LASSO_nonzero_features.to_csv(folder_dir +\n",
    "                                 '/LASSO_coefficients/LASSO_all_nonzero_features_Augmented_ROBIN_RNA_vs_BindingDB.csv',\n",
    "                                 index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294687f",
   "metadata": {},
   "source": [
    "# MLP Neural Network Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the neural network packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define the neural network MLP model and wrap it in the scikitlearn pipeline for easier use\n",
    "\n",
    "# Define neural network as function\n",
    "def create_model():\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Define number of features\n",
    "    number_of_features = 1665\n",
    "\n",
    "    # Hidden layers\n",
    "    model.add(layers.Dense(units=200, activation='relu', kernel_regularizer=regularizers.l2(0.00001),\n",
    "                           input_shape=(number_of_features,)))\n",
    "\n",
    "    model.add(layers.Dense(units=200, activation='relu', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',  # Cross-entropy\n",
    "                  optimizer='Adam',  # Use Adam optimizer\n",
    "                  metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Wrap within the scikitlearn framework\n",
    "MLP_model = KerasClassifier(create_model, epochs=7, batch_size=2048, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccedf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the MLP neural network model with the same cross-validation strategy\n",
    "\n",
    "# Import required packages for the PR curve\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Import the package for adjusting class weights for the neural net\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "# Set plot font to Halvetica\n",
    "plt.rc('font', family='Helvetica')\n",
    "\n",
    "# Set random seed for numpy and tensorflow\n",
    "RANDOM_SEED = 5\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Make sure matplotlib settings are reset\n",
    "plt.style.use('default')\n",
    "\n",
    "# Define the colors to be used for the 10 folds as they are drawn on the PR curve\n",
    "colors = [\"lightblue\", \"r\", \"c\", \"m\", \"y\", \"lime\", \"mediumpurple\", \"olive\", \"aqua\",\n",
    "          \"magenta\", \"skyblue\", \"yellowgreen\", \"gold\", \"tan\", \"silver\"]\n",
    "\n",
    "# Cross-validation for loop\n",
    "MLP_y_real = []\n",
    "MLP_y_proba = []\n",
    "num = 0\n",
    "for k in range(1, 11):\n",
    "\n",
    "    # Print fold number\n",
    "    print(\"Fold \" + str(num+1) + \":\")\n",
    "\n",
    "    # Read in the fetures and labels for the train and test sets read earlier\n",
    "    X_train = vars()['fold_' + str(k) + \"_X_train\"]\n",
    "    y_train = vars()['fold_' + str(k) + \"_y_train\"]\n",
    "    X_test = vars()['fold_' + str(k) + \"_X_test\"]\n",
    "    y_test = vars()['fold_' + str(k) + \"_y_test\"]\n",
    "\n",
    "    # Update num\n",
    "    num += 1\n",
    "\n",
    "    # Convert the dataframes to numpy arrays for machine learning\n",
    "    X_3_all_Augmented_train = X_train.values\n",
    "    y_3_all_Augmented_train = y_train.values\n",
    "    X_3_all_Augmented_test = X_test.values\n",
    "    y_3_all_Augmented_test = y_test.values\n",
    "\n",
    "    # There is still a slight imbalance in the train set so adjust the weights accordingly\n",
    "    y_3_all_Augmented_train_list = [x[0] for x in y_3_all_Augmented_train.tolist()]\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                                        classes=np.unique(y_3_all_Augmented_train_list),\n",
    "                                        y=y_3_all_Augmented_train_list)\n",
    "    class_weights = dict(zip(np.unique(y_3_all_Augmented_train_list), class_weights))\n",
    "    print(\"Class weights:\")\n",
    "    print(class_weights)\n",
    "    print()\n",
    "\n",
    "    # Fit the model on the training set and predict probabilities for the train and test sets\n",
    "    MLP_fit = MLP_model.fit(X_3_all_Augmented_train, y_3_all_Augmented_train, class_weight=class_weights)\n",
    "    MLP_test_probas = MLP_model.predict_proba(X_3_all_Augmented_test)\n",
    "    MLP_train_probas = MLP_model.predict_proba(X_3_all_Augmented_train)\n",
    "\n",
    "    # Compute PR and area under the curve\n",
    "    precision, recall, _ = precision_recall_curve(y_3_all_Augmented_test, MLP_test_probas[:, 1])\n",
    "\n",
    "    # Plot each individual PR Curve and append to MLP_y_real and MLP_y_proba lists\n",
    "    plt.plot(recall, precision, color=colors[num], lw=1, alpha=0.2)\n",
    "    MLP_y_real.append(y_3_all_Augmented_test)\n",
    "    MLP_y_proba.append(MLP_test_probas[:, 1])\n",
    "\n",
    "    # Get model class predictions instead of probabilities to show the confusion matrices\n",
    "    prediction_classes_test = MLP_model.predict(X_3_all_Augmented_test)\n",
    "    prediction_classes_train = MLP_model.predict(X_3_all_Augmented_train)\n",
    "\n",
    "    # Draw confusion matrix of the model performance on the train set\n",
    "    cm = confusion_matrix(y_3_all_Augmented_train, prediction_classes_train)\n",
    "    print(\"Confusion matrix on the test set:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Draw confusion matrix of the model performance on the test set\n",
    "    cm = confusion_matrix(y_3_all_Augmented_test, prediction_classes_test)\n",
    "    print(\"Confusion matrix on the test set:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Print F1 score and average prevision score for train and test sets in each round\n",
    "    print(\"F1 score test:\")\n",
    "    print(round(f1_score(y_3_all_Augmented_test, prediction_classes_test), 2))\n",
    "    print(\"F1 score train:\")\n",
    "    print(round(f1_score(y_3_all_Augmented_train, prediction_classes_train), 2))\n",
    "    print()\n",
    "    print(\"Test AVG precision score:\")\n",
    "    print(round(average_precision_score(y_3_all_Augmented_test, MLP_test_probas[:, 1]), 2))\n",
    "    print(\"Train AVG precision score:\")\n",
    "    print(round(average_precision_score(y_3_all_Augmented_train, MLP_train_probas[:, 1]), 2))\n",
    "    print()\n",
    "    print(\"*******************\")\n",
    "    print()\n",
    "\n",
    "# Plot the no skill line on the bottom of the PR curve\n",
    "no_skill = len(y_all_4_CV[y_all_4_CV == 1]) / len(y_all_4_CV)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill', color='r')\n",
    "\n",
    "# Concatenate all MLP_y_real and MLP_y_proba arrays for drawing of the mean curve\n",
    "MLP_y_real = np.concatenate(MLP_y_real)\n",
    "MLP_y_proba = np.concatenate(MLP_y_proba)\n",
    "\n",
    "# Calculate mean precision and recall for drawing the PR curve in the next step\n",
    "MLP_precision, MLP_recall, thresholds = precision_recall_curve(MLP_y_real, MLP_y_proba)\n",
    "\n",
    "# Plot the mean PR curve\n",
    "plt.plot(MLP_recall, MLP_precision, color='b',\n",
    "             label=r'MLP: Mean AUPRC (K=10) = %0.2f' % (average_precision_score(MLP_y_real, MLP_y_proba)),\n",
    "             lw=2, alpha=1)\n",
    "\n",
    "# Set aspect ratio\n",
    "axes = plt.gca()\n",
    "axes.set_aspect(0.75)\n",
    "\n",
    "# Remove the gridlines\n",
    "plt.grid(visible=False)\n",
    "\n",
    "\n",
    "# Set x-axis and y-axis ticks\n",
    "tick_list = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "plt.xticks(tick_list, tick_list, fontsize=15)\n",
    "plt.yticks(tick_list, tick_list, fontsize=15)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.legend(loc=\"lower left\", fontsize=13)\n",
    "\n",
    "\n",
    "# Save the PR curve\n",
    "plt.savefig(folder_dir + '/figures/PRC_Augmented_ROBIN_RNA_vs_BindingDB_MLP.png', dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of the MLP model on the holdout/test set and report relevant metrics\n",
    "\n",
    "# Import packages required for calculation of different metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Set random seed for numpy and tensorflow\n",
    "RANDOM_SEED = 5\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Print initial shape of df_all_3 for reference\n",
    "print(\"Shape of df_all_3:\")\n",
    "print(df_all_3.shape)\n",
    "print()\n",
    "\n",
    "'''\n",
    "The same strategy as cross validation is used here but instead of a cross validation test set, X_all_4_holdout\n",
    "contains compounds of the test set.\n",
    "'''\n",
    "\n",
    "'''\n",
    "The standardization strategy is different here. Earlier, for both train and test sets, we used features from the\n",
    "df_all_3_name_label_standardized matrix which was the entire df_all_3 standardized. This approach was used\n",
    "to ensure we have the same standardization of features for all cross validation folds. For testing on the holdout\n",
    "set here, we standardize the train set separately from the test set to ensure no leakage occurs from the test\n",
    "set being standardized in the same set as the train set. We do this by taking feature values directly from\n",
    "df_all_3 and standardizing after the train/test split.\n",
    "'''\n",
    "\n",
    "\n",
    "# RNA compounds to exclude from the training set\n",
    "test_set_compounds = X_all_4_holdout[:, 0].tolist()\n",
    "test_set_RNA_compounds = [item for item in test_set_compounds if item[0:9] != \"BindingDB\"]\n",
    "test_set_RNA_compounds_expanded = []\n",
    "for item in test_set_RNA_compounds:\n",
    "    test_set_RNA_compounds_expanded.append(item)\n",
    "    item_expanded = ROBIN_Augment_dict[item]\n",
    "    for thing in item_expanded:\n",
    "        test_set_RNA_compounds_expanded.append(thing)\n",
    "test_set_RNA_compounds_expanded_set = list(set(test_set_RNA_compounds_expanded))\n",
    "\n",
    "# BindingDB compounds to exclude from the training set\n",
    "test_set_BindingDB = [item for item in test_set_compounds if item[0:9] == \"BindingDB\"]\n",
    "\n",
    "# BindingDB and RNA binders to be excluded from the train set combined into one set\n",
    "test_set_RNA_expanded_and_BindingDB = test_set_BindingDB + test_set_RNA_compounds_expanded_set\n",
    "\n",
    "# Remove from df_all_3_name_label_standardized_train BindingDB compounds from the test set\n",
    "# Also remove from df_all_3_name_label_standardized_train tes set ROBIN RNA binders and their analogues\n",
    "df_all_3_train = df_all_3.loc[~df_all_3['name'].isin(test_set_RNA_expanded_and_BindingDB)]\n",
    "# df_all_3_name_label_standardized_train is the train set\n",
    "\n",
    "# Print shape of the train set and the number of protein and RNA binders within it\n",
    "print(\"Shape of the augmented train set:\")\n",
    "print(df_all_3_train.shape)\n",
    "y_all_3_train_labels_list = df_all_3_train.values[:, -1].tolist()\n",
    "y_all_3_train_labels_list = [1 if x == \"RNA Binding\" else x for x in y_all_3_train_labels_list]\n",
    "y_all_3_train_labels_list = [0 if x == \"Protein Binding\" else x for x in y_all_3_train_labels_list]\n",
    "print(\"Number of protein binders:\")\n",
    "print(int(len(y_all_3_train_labels_list) - sum(y_all_3_train_labels_list)))\n",
    "print(\"Number of RNA binders:\")\n",
    "print(int(sum(y_all_3_train_labels_list)))\n",
    "print()\n",
    "\n",
    "# Extract from df_all_3_name_label_standardized the test set compounds belonging to BindingDB and ROBIN RNA binders\n",
    "test_set_RNA_and_BindingDB = test_set_RNA_compounds + test_set_BindingDB\n",
    "df_all_3_test = df_all_3.loc[df_all_3['name'].isin(test_set_RNA_and_BindingDB)]\n",
    "df_all_3_test.drop_duplicates(subset=['name'], keep=\"first\", inplace=True)\n",
    "# df_all_3_name_label_standardized_test is the test set\n",
    "# Test set compounds are extracted again from df_all_3_name_label_standardized to ensure standardization of train and test sets are the same\n",
    "\n",
    "# Print shape of the test set and the number of protein and RNA binders within it\n",
    "print(\"Shape of the test set:\")\n",
    "print(df_all_3_test.shape)\n",
    "y_all_3_test_labels_list = df_all_3_test.values[:, -1].tolist()\n",
    "y_all_3_test_labels_list = [1 if x == \"RNA Binding\" else x for x in y_all_3_test_labels_list]\n",
    "y_all_3_test_labels_list = [0 if x == \"Protein Binding\" else x for x in y_all_3_test_labels_list]\n",
    "print(\"Number of protein binders:\")\n",
    "print(int(len(y_all_3_test_labels_list) - sum(y_all_3_test_labels_list)))\n",
    "print(\"Number of RNA binders:\")\n",
    "print(int(sum(y_all_3_test_labels_list)))\n",
    "print()\n",
    "\n",
    "\n",
    "# Prepare the X_train matrix and standardize as described earlier\n",
    "X_3_all_Augmented_train = df_all_3_train.values[:, 1:-1]\n",
    "X_3_all_Augmented_train = X_3_all_Augmented_train.astype('float64')\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_3_all_Augmented_train)\n",
    "X_3_all_Augmented_train = sc.transform(X_3_all_Augmented_train)\n",
    "print(\"Shape of X_3_all_Augmented_train matrix:\")\n",
    "print(X_3_all_Augmented_train.shape)\n",
    "print()\n",
    "\n",
    "# Prepare the X_test matrix and standardize as described earlier\n",
    "X_3_all_Augmented_test = df_all_3_test.values[:, 1:-1]\n",
    "X_3_all_Augmented_test = X_3_all_Augmented_test.astype('float64')\n",
    "X_3_all_Augmented_test = sc.transform(X_3_all_Augmented_test)\n",
    "print(\"Shape of X_3_all_Augmented_test matrix:\")\n",
    "print(X_3_all_Augmented_test.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Prepare the y_train and y_test matrices\n",
    "y_3_all_Augmented_train = y_all_3_train_labels_list\n",
    "y_3_all_Augmented_test = y_all_3_test_labels_list\n",
    "\n",
    "# Adjust class weights for the MLP model\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                                        classes=np.unique(y_3_all_Augmented_train),\n",
    "                                        y=y_3_all_Augmented_train)\n",
    "class_weights = dict(zip(np.unique(y_3_all_Augmented_train), class_weights))\n",
    "print(\"Class_weights:\")\n",
    "print(class_weights)\n",
    "\n",
    "# Fit MLP model on the train set\n",
    "MLP_fit = MLP_model.fit(X_3_all_Augmented_train, y_3_all_Augmented_train, class_weight=class_weights)\n",
    "\n",
    "# Calculate probabilities for the test set\n",
    "prediction_probs = MLP_model.predict_proba(X_3_all_Augmented_test)[:, 1]\n",
    "\n",
    "# Calculate classes for the test set\n",
    "prediction_classes = MLP_model.predict(X_3_all_Augmented_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_3_all_Augmented_test, prediction_classes)\n",
    "print(\"Confusion matrix on the holdout set:\")\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Calculate average precision score\n",
    "AUPRC_score_holdout = average_precision_score(y_3_all_Augmented_test, prediction_probs)\n",
    "print(\"AUPRC score on the holdout set:\")\n",
    "print(round(AUPRC_score_holdout, 2))\n",
    "print()\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score_holdout = f1_score(y_3_all_Augmented_test, prediction_classes)\n",
    "print(\"F1 score on the holdout set:\")\n",
    "print(round(f1_score_holdout, 2))\n",
    "print()\n",
    "\n",
    "# Calculate AUROC score\n",
    "AUROC_score_holdout = roc_auc_score(y_3_all_Augmented_test, prediction_probs)\n",
    "print(\"AUROC score on the holdout set:\")\n",
    "print(round(AUROC_score_holdout, 2))\n",
    "print()\n",
    "\n",
    "# Calculate Cohen's kappa\n",
    "cohen_kappa_score_holdout = cohen_kappa_score(y_3_all_Augmented_test, prediction_classes)\n",
    "print(\"Cohenâ€™s kappa score on the holdout set:\")\n",
    "print(round(cohen_kappa_score_holdout, 2))\n",
    "print()\n",
    "\n",
    "# Calculate Matthews correlation coefficient (MCC)\n",
    "matthews_corrcoef_holdout = matthews_corrcoef(y_3_all_Augmented_test, prediction_classes)\n",
    "print(\"Matthews correlation coefficient on the holdout set:\")\n",
    "print(round(matthews_corrcoef_holdout, 2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00624a17",
   "metadata": {},
   "source": [
    "## Test MLP on four RNA binders and four protein binders from the literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6976c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract df_all_3 column names and their median feature value as a dictionary to fill nan values of the test matrix\n",
    "\n",
    "df_all_3_values = df_all_3.iloc[:, 1:-1] \n",
    "df_all_3_features = df_all_3_values.columns.tolist()\n",
    "d_df_all_3_feature_medians = {}\n",
    "for item in df_all_3_features:\n",
    "    d_df_all_3_feature_medians[item] = df_all_3_values[item].median()\n",
    "d_df_all_3_feature_medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of the neural net on several cases from the scientific literature\n",
    "\n",
    "# Read in Mordred features of the test compounds\n",
    "df_Mordred_extra = pd.read_csv(folder_dir + \"/data/Mordred_files/Mordred_Test_Compounds_3D.csv\")\n",
    "# Take the compound names as a list as they will be used later\n",
    "Mordred_extra_names = df_Mordred_extra[\"name\"].tolist()\n",
    "# Remove the name column\n",
    "df_Mordred_extra_no_name = df_Mordred_extra.iloc[:, 1:]\n",
    "# Only keep features of test compounds that also exist in df_all_3\n",
    "features = df_all_3.columns.tolist()[1:-1]\n",
    "for item in df_Mordred_extra_no_name:\n",
    "    if item not in features:\n",
    "        df_Mordred_extra_no_name = df_Mordred_extra_no_name.drop(item, axis=1)\n",
    "\n",
    "# Replace infinity in the feature values by nan values\n",
    "df_Mordred_extra_no_name.replace([np.inf, -np.inf], np.nan, inplace=True)     \n",
    "\n",
    "# Fill nan values of the test matrix with median of df_all_3 columns\n",
    "for i in range(8):\n",
    "    for item in features:\n",
    "        if np.isnan(df_Mordred_extra_no_name.at[i, item]) == True:\n",
    "            df_Mordred_extra_no_name.at[i, item] = d_df_all_3_feature_medians[item]\n",
    "            \n",
    "df_Mordred_extra_no_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform standardization on the train(the entire df_all_3 dataset) and test (12 compounds) sets.\n",
    "\n",
    "# Import standardization package\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# remove name column from df_all_3\n",
    "df_all_3_no_name = df_all_3.iloc[:, 1:]\n",
    "\n",
    "# Convert both df_all_3 and test dataframes to numpy arrays for ML\n",
    "X_all_3 = df_all_3_no_name.values[:, :-1]\n",
    "X_extra_test = df_Mordred_extra_no_name.values\n",
    "y_all_3 = df_all_3_no_name.iloc[:, -1].tolist()\n",
    "\n",
    "#print shapes of X_all_3, y_all_3, and X_extra_test\n",
    "print (X_all_3.shape)\n",
    "print (X_extra_test.shape)\n",
    "print (len(y_all_3))\n",
    "print ()\n",
    "\n",
    "# Standardize X_all_3\n",
    "X_all_3 = X_all_3.astype('float64')\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_all_3)\n",
    "X_all_3 = sc.transform(X_all_3)\n",
    "\n",
    "# Standardize X_extra_test using the X_all_3 scaling\n",
    "# Train and test sets are stadardized separately again to prevent data leakage\n",
    "X_extra_test = sc.transform(X_extra_test)\n",
    "\n",
    "# Prepare y_all_3\n",
    "y_all_3 = [1 if x==\"RNA Binding\" else x for x in y_all_3]\n",
    "y_all_3 = [0 if x==\"Protein Binding\" else x for x in y_all_3]\n",
    "y_all_3 = np.array(y_all_3)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neural network MLP model (same as before) is repeated here for reference.\n",
    "\n",
    "# Import the neural network packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define the neural network MLP model and wrap it in the scikitlearn pipeline for easier use\n",
    "\n",
    "# Define neural network as function\n",
    "def create_model():\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Define number of features\n",
    "    number_of_features = 1665\n",
    "\n",
    "    # Hidden layers\n",
    "    model.add(layers.Dense(units=200, activation='relu', kernel_regularizer=regularizers.l2(0.00001),\n",
    "                           input_shape=(number_of_features,)))\n",
    "\n",
    "    model.add(layers.Dense(units=200, activation='relu', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',  # Cross-entropy\n",
    "                  optimizer='Adam',  # Use Adam optimizer\n",
    "                  metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Wrap within the scikitlearn framework\n",
    "MLP_model = KerasClassifier(create_model, epochs=7, batch_size=2048, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MLP model on X_all_3 and y_all_3 and test on X_extra_test\n",
    "\n",
    "# Set random seed for numpy and tensorflow\n",
    "RANDOM_SEED = 5\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Make sure the slight imbalance in the training set is accounted for in the neural network\n",
    "class_weights = compute_class_weight(class_weight = \"balanced\",\n",
    "                                        classes = np.unique(y_all_3),\n",
    "                                        y = y_all_3)\n",
    "class_weights = dict(zip(np.unique(y_all_3), class_weights))\n",
    "print (class_weights)\n",
    "\n",
    "\n",
    "# Fit the MLP model to the training set\n",
    "MLP_fit = MLP_model.fit(X_all_3, y_all_3)\n",
    "\n",
    "# Predict probabilities for X_extra_test\n",
    "prediction_extra = MLP_model.predict_proba(X_extra_test)\n",
    "\n",
    "# Print compound name followed by the probability predicted by model\n",
    "for i in range(len(prediction_extra)):\n",
    "    print (Mordred_extra_names[i])\n",
    "    print (round(prediction_extra[i][1]*100, 1))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196f31e",
   "metadata": {},
   "source": [
    "## SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6abb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the model on the entire dataset in preparation for the SHAP analysis\n",
    "\n",
    "model = Sequential()\n",
    "number_of_features = 1665\n",
    "\n",
    "model.add(layers.Dense(units=200, activation='relu', kernel_regularizer=regularizers.l2(0.00001),input_shape=(number_of_features,)))\n",
    "\n",
    "model.add(layers.Dense(units=200, activation='relu', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "\n",
    "model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='Adam', \n",
    "                  metrics=['AUC'])\n",
    "\n",
    "history = model.fit(X_all_3, y_all_3, epochs=7, batch_size=2048, verbose=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform SHAP to interpret the neural network model\n",
    "\n",
    "# Import shap package\n",
    "import shap\n",
    "\n",
    "#initialize\n",
    "shap.initjs()\n",
    "\n",
    "#define the background \n",
    "background = X_all_3[np.random.choice(X_all_3.shape[0], 1000, replace=False)]\n",
    "\n",
    "#use the deepexplainer package within SHAP tailored for use in neural networks\n",
    "e = shap.DeepExplainer(model, background)\n",
    "\n",
    "#calculate SHAP values\n",
    "shap_values = e.shap_values(X_all_3)\n",
    "\n",
    "#print SHAP values\n",
    "print(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afa2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SHAP values to CSV as the SHAP code takes a long time to run\n",
    "df_shap_save = pd.DataFrame(shap_values[0])\n",
    "df_shap_save.to_csv(folder_dir + \"/ML_CSVs/SHAP_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a03ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back in the saved CSV containing the SHAP values\n",
    "df_SHAP_array = pd.read_csv(folder_dir + \"/ML_CSVs/Shap_values.csv\")\n",
    "df_SHAP_array = df_SHAP_array.drop('Unnamed: 0', axis=1)\n",
    "df_SHAP_array = df_SHAP_array.to_numpy()\n",
    "print(df_SHAP_array.shape)\n",
    "df_SHAP_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad8c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the SHAP beeswarm plot\n",
    "\n",
    "# Set plot font to Halvetica\n",
    "plt.rc('font', family='Helvetica') \n",
    "\n",
    "df_SHAP = df_all_3.drop('library', axis=1)\n",
    "df_SHAP = df_SHAP.drop('name', axis=1)\n",
    "shap.summary_plot(df_SHAP_array, df_SHAP, show=False)\n",
    "plt.savefig(folder_dir + '/figures/SHAP_Beeswarm_plot.png', bbox_inches=\"tight\" ,dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the barplot for the SHAP beeswarm plot\n",
    "\n",
    "shap.summary_plot(shap_values, df_SHAP, show=False)\n",
    "plt.savefig(folder_dir + '/figures/SHAP_Barplot.png', bbox_inches=\"tight\" ,dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6fa73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735a9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce8d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbacf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f83b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdb440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e15d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb5ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31679f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
